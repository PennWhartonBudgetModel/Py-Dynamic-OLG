##
# Reader and generator of parameters for dynamic model.
#

from pathFinderModule import PathFinder
from inputReaderModule import InputReader
from scenarioModule import Scenario

import numpy as np
import pandas as pd
from scipy.stats import norm
from scipy import integrate
import scipy
import math
import warnings
import os

class ParamGenerator:
    
    # TIMING
    #     Includes policy-specific SS.NRA
    @staticmethod
    def timing(scenario):
        
        s = {}
        s['realage_entry']         = 20    # Real age of model age=0   
        s['T_life']                = 80    # Death year
        s['Tmax_work']             = 52    # Last possible year of working age     
        
        # Time range for transition path
        s['TransitionFirstYear'] = scenario.TransitionFirstYear
        s['TransitionLastYear']  = scenario.TransitionLastYear
        
        if scenario.isSteady():
            s['T_model']       = 1        # Steady state total modeling years
            s['startyears']    = np.zeros(1)        # Steady state cohort start year
            s['yearFirst']    = scenario.TransitionFirstYear - 1
            s['yearLast']     = s['yearFirst']
        else:
            s['yearFirst']     = scenario.TransitionFirstYear
            s['yearLast']      = scenario.TransitionLastYear - 1
            s['T_model']       = s['TransitionLastYear'] - s['TransitionFirstYear']
            s['startyears']    = np.array(range((-s['T_life']+1), s['T_model'])) # Transition path cohort start years
            
        return s
    
    ## IMPORT GRIDS
    #   Import labor_productivity (grids, transition matrices, and initial
    #       household distribution) from csv files
    #   TBD: Import assets and average earnings grids
    @staticmethod
    def grids(scenario):
        
        timing    = ParamGenerator.timing(scenario)
        T_life    = timing['T_life']
        yearFirst = timing['yearFirst']
        yearLast  = timing['yearLast']
           
        pathFinder = PathFinder(scenario)
        
        #
        # TODO: We are working toward replacing the shock generation with
        #      a calibrated (from PWBMsim data) transition.
        #       As an incremental step, we import values from a csv file 
        #       generated by the function labinc_discretization.
        
        file        = pathFinder.getLaborShockInputPath( 'labor_productivity' )
        shocks      = InputReader.read_shocks(file, yearFirst,(yearLast - yearFirst + 1), T_life)
        file        = pathFinder.getLaborShockInputPath( 'transitions' )
        transitions = InputReader.read_transitions(file, yearFirst,(yearLast - yearFirst + 1), T_life)
        file        = pathFinder.getLaborShockInputPath( 'initial_DIST' )
        DISTz       = InputReader.read_initDIST(file, yearFirst, T_life)
        
        # Checks
        nz = shocks.shape[2]
        assert transitions.shape[2] == nz, 'Transition matrix and shocks vector have incompatible dimensions'
        assert transitions.shape[3] == nz, 'Transition matrix and shocks vector have incompatible dimensions'
        assert DISTz.shape[0] == nz, 'Initial distribution and shocks vector have incompatible dimensions'
        
        # Define population group index mapping
        groups = ['citizen', 'legal', 'illegal']
        ng = len(groups)
        g = dict(zip(groups, list(range(0,ng))))
        
        assert ng == DISTz.shape[2], 'WARNING! Initial distribution file does not contain %u groups.' % ng
        
        # Asset grid: built to range from $20.00 to $150 million
        f  = lambda lb, ub, n, curv: lb + (ub-lb)*(np.array(list(range(n)))/(n-1))**curv
        nk = 12                              # number of total grid points for asset grid
        k1 = 20                              # first grid point value $20.00
        kn = 1500000                         # last grid point in function f value $1,500,000.00
        kv = f(k1, kn, nk-4, 4.1)
        
        # Additions to asset grid
        scale = 1e+6                         # scaling parameter to continue building the capital grid 
        for ik in range(nk-4,nk):                      # this loop builds the top of capital grid such that:
            scale = 3.5*scale                # i.  it re-starts at 3.5 million dollars
            kv = np.append(kv, scale)                   # ii. and its last point is around 150 million dollars 
    
        kv = kv*scenario.modelunit_dollar    # Converting from dollars to model units       
        
        # Average earnings grid
        ssincmax = 1.185e5                                                  # 118,500 is the maximum annual labor income for Social Security tax purposes
        nb =  5
        bv = f(0, ssincmax*scenario.modelunit_dollar, nb  , 2)  # average earnings vector --- Upper bound of average earnings defined as maximum possible Social Security benefit
        bv = np.append(bv, 15*bv[-1])                                               # max point arbitrarily set to 15x the second largest one
        nb = nb + 1
        
        s = {}
        s['g']        = g
        s['ng']       = ng
        s['nz']       = nz
        s['transz']   = transitions
        s['DISTz']    = DISTz
        s['zs']       = shocks
        s['nk']       = nk
        s['kv']       = kv
        s['nb']       = nb
        s['bv']       = bv
        
        return s
    
    
    
    ## LABOR INCOME PROCESS DISCRETIZATION
    #   Generate grids for labor productivity
    #   (grids, transition matrices, and initial household distribution)
    @staticmethod
    def labinc_discretization(scenario):

        timing       = ParamGenerator.timing(scenario)
        T_life       = timing['T_life']
        T_workMax    = timing['Tmax_work']
        ndem         = 1                       # initialize number of demographic types
        prem_legal   = scenario.prem_legal     # productivity premium of avg. (by year) legal immigrant. Note: This is a ratio.
        
        # Using the old demographics, we'll use the standard 0.9, when
        # using the new demographics, we'll use the scenario parameter, TBD
        # calibrated
        if scenario.UseNewDemographics:
            prem_illegal = scenario.prem_illegal   # productivity premium of avg. (by year) illegal immigrant. Note: This is a ratio by PWBM at \\SHARED_DRIVE\PWBM_MicroSIM\Outputs\StructuralModelInputs TODO: The real value is 0.62043 -- get value from microsim
        else:
            prem_illegal = 0.9
            
        # Define population group index mapping
        g = {}
        groups = ['citizen', 'legal', 'illegal']
        ng = len(groups)
        for ig in range(ng):
            g[groups[ig]] = ig
        
        if scenario.LaborShock == 'MichaelJordan10':
            
            # (Shock process definitions from Storesletten, Telmer, and Yaron, 2004)
            # (Method of discretizing shock process derived from Adda and Cooper, 2003)
        
            # Define function to compute cutoff points for equally weighted slices of a normal distribution
            f = lambda n, sig: n*sig*-np.diff(norm.pdf(norm.ppf(np.linspace(0, 1, num=n+1))))
        
            # Determine permanent and transitory shocks
            nperm  = 2
            zperm  = f(nperm , math.sqrt(0.2105))
            ntrans = 2
            ztrans = f(ntrans, math.sqrt(0.0630))
        
            # Determine persistent shocks
            npers = 2
            pep = 0.990                       # Lagged productivity coefficient
            sigep = math.sqrt(0.018)             # Conditional standard deviation of productivity
            sigpers = sigep/(math.sqrt(1-pep**2))
            zpers = f(npers, sigpers)
        
            # Construct Markov transition matrix for persistent shocks by approximating an AR(1) process
            persv = sigpers*norm.ppf(np.linspace(0, 1, n=npers+1))
            transpers = np.zeros((npers,npers))
            for ipers in range(npers):
                integrand = lambda x: np.exp(-x**2/(2*sigpers**2)) * np.diff(norm.cdf((persv[ipers-1:ipers+1] - pep*x)/sigep))
                for jpers in range(npers):
                    f = lambda v: [integrand(x) for x in v]
                    transpers[ipers,jpers] = (npers/(math.sqrt(2*math.pi*(sigpers**2)))) * integrate.quad(f , persv[jpers], persv[jpers+1])
            
            # Determine initial distribution over persistent shocks
            DISTpers = np.diff(norm.cdf(persv/math.sqrt(0.124)))
        
            # Define deterministic lifecycle productivities
            timing    = ParamGenerator.timing(scenario)
            T_life    = timing['T_life']
            T_workMax = timing['Tmax_work']
        
            # Life-cycle productivity from Conesa et al. 2017 - average for healthy workers
            pathFinder = PathFinder(scenario)
            file      = pathFinder.getMicrosimInputPath( 'ConesaEtAl_WageAgeProfile' )
            series    = InputReader.read_series(file, 'Age', 1 + timing['realage_entry'], [])
            zage      = series['Wage']
        
            # Calculate total productivity
            ndem = nperm
            nz = ntrans*npers
        
            zs = np.maximum(0, np.tile(np.reshape(zage, (1 ,T_life,1)), [nz,1,ndem])
                  + np.tile(np.reshape(zperm, (1, 1, ndem)), [nz,T_life,1]) 
                  + np.tile(np.reshape(np.kron(np.ones((1,npers)), ztrans)
                  + np.kron(zpers, np.ones((1,ntrans))), (nz,1,1)), [1 ,T_life,ndem]))

            zs_check = zs[:,0:T_workMax,:]
            assert np.all(zs_check[:] > 0), 'WARNING! Productivity shock grid contains zero.'
        
            # Construct Markov transition matrix for all shocks / productivities
            transz = np.kron(transpers, (1/ntrans) * np.ones((ntrans,ntrans)))
        
            # Determine initial distribution over all shocks / productivities
            DISTz0 = np.kron(DISTpers, (1/ntrans) * np.ones((1, ntrans)))

            # Include a fifth super large and rare shock
            nz = 5
            zs[5,:,:] = zs[4,:,:] * 15
            transz = np.array([[transz[0,0], transz[0,1], transz[0,2], transz[0,3], 0.00],
                  [transz[1,0], transz[1,1], 0.02,        0.02,        (1-(transz[1,0]+transz[1,1])-2*0.02)],
                  [transz[2,0], transz[2,1], 0.47,        0.47,        (1-(transz[2,1]+transz[2,1])-2*0.47)],
                  [0.03,        0.03,        0.47,        0.46,         0.01],                    
                  [0.15,        0.05,        0.05,        0.25,         0.50]])
            DISTz0.append(0)
            
            # Determine productivity distributions for ages
            DISTz_g      = np.zeros(nz,T_life)
            DISTz_g[:,0] = np.reshape(DISTz0, (nz,1))
        
            for age in range(1,T_life):
                DISTz_g[:,age] = np.matmul( np.transpose(transz) , DISTz_g[:,age-1])
        
        
            # Replicate age productivity distributions for population groups
            DISTz = np.tile(DISTz_g[:,:,np.newaxis], [1,1,ng])
            
            # Shift productivity distributions for legal and illegal immigrants
            # towards highest and lowest productivity levels respectively
            for age in range(T_life):
            
                zmean = np.mean(zs[:,age,:], axis = 2)
            
                if not np.array_equal(zmean * DISTz[:,age,g['citizen']] , zmean * DISTz[:,age,g['legal']] * prem_legal) :
                    zlegal   = sum(zmean * DISTz[:,age,g['legal']]) * prem_legal 
                    plegal   = (zmean[nz] - zlegal  ) / (zmean[nz]*(nz-1) - sum(zmean[0:nz-1]))
                    DISTz[:,age,g['legal']] = np.vstack((plegal*np.ones(nz-1,1), 1 - plegal*(nz-1)))

                zillegal = sum(zmean * DISTz[:,age,g['illegal']]) * prem_illegal
                pillegal = (zmean[0]  - zillegal) / (zmean[0] *(nz-1) - sum(zmean[1:nz]))
                DISTz[:,age,g['illegal']] = np.vstack((1 - pillegal*(nz-1), pillegal*np.ones((nz-1,1))))
            
            # Make the larger sized transz grid and replicate the current transz grid.
            transz      = np.vstack((np.hstack((transz, np.zeros[nz])), np.hstack((np.zeros[nz], transz))))
            transitions = np.tile(transz, [1, 1, T_life])
            zs          = np.vstack((zs[:,:,0], zs[:,:,1]))
            DISTz       = np.vstack((DISTz, DISTz)) / 2

        elif scenario.LaborShock == 'MichaelJordan12':
            
            # (Shock process definitions from Storesletten, Telmer, and Yaron, 2004)
            # (Method of discretizing shock process derived from Adda and Cooper, 2003)
        
            # Define function to compute cutoff points for equally weighted slices of a normal distribution
            f = lambda n, sig: n*sig*-np.diff(norm.pdf(norm.ppf(np.linspace(0, 1, num=n+1))))
        
            # Determine permanent and transitory shocks
            nperm  = 2
            zperm  = f(nperm , math.sqrt(0.2105))
            ntrans = 2
            ztrans = f(ntrans, math.sqrt(0.0630))
        
            # Determine persistent shocks
            npers = 2
            pep = 0.990                       # Lagged productivity coefficient
            sigep = math.sqrt(0.018)             # Conditional standard deviation of productivity
            sigpers = sigep/(math.sqrt(1-pep**2))
            zpers = f(npers, sigpers)
            
            # Construct Markov transition matrix for persistent shocks by approximating an AR(1) process
            persv = sigpers*norm.ppf(np.linspace(0, 1, n=npers+1))
            transpers = np.zeros((npers,npers))
            for ipers in range(npers):
                integrand = lambda x: np.exp(-x**2/(2*sigpers**2)) * np.diff(norm.cdf((persv[ipers-1:ipers+1] - pep*x)/sigep))
                for jpers in range(npers):
                    f = lambda v: [integrand(x) for x in v]
                    transpers[ipers,jpers] = (npers/(math.sqrt(2*math.pi*(sigpers**2)))) * integrate.quad(f , persv[jpers], persv[jpers+1])
            
            # Determine initial distribution over persistent shocks
            DISTpers = np.diff(norm.cdf(persv/math.sqrt(0.124)))
            
            # Life-cycle productivity from Conesa et al. 2017 - average for healthy workers
            pathFinder = PathFinder(scenario)
            file      = pathFinder.getMicrosimInputPath( 'ConesaEtAl_WageAgeProfile' )
            series    = InputReader.read_series(file, 'Age', 1 + timing['realage_entry'], [])
            zage      = series['Wage']
            
             # Calculate total productivity
            ndem = nperm
            nz = ntrans*npers
        
            zs = np.maximum(0, np.tile(np.reshape(zage, (1 ,T_life,1)), [nz,1,ndem])
                  + np.tile(np.reshape(zperm, (1, 1, ndem)), [nz,T_life,1]) 
                  + np.tile(np.reshape(np.kron(np.ones((1,npers)), ztrans)
                  + np.kron(zpers, np.ones((1,ntrans))), (nz,1,1)), [1 ,T_life,ndem]))

            zs_check = zs[:,0:T_workMax,:]
            assert np.all(zs_check[:] > 0), 'WARNING! Productivity shock grid contains zero.'
        
            # Construct Markov transition matrix for all shocks / productivities
            transz = np.kron(transpers, (1/ntrans) * np.ones((ntrans,ntrans)))
        
            # Determine initial distribution over all shocks / productivities
            DISTz0 = np.kron(DISTpers, (1/ntrans) * np.ones((1, ntrans)))

            # Include a large shock (middle class) and a super large
            # and rare shock (super rich)
            nz = 6
            zs[4,:,:] = zs[3,:,:] * 3
            zs[5,:,:] = zs[3,:,:] * 30
            zs[3,:,:] = zs[3,:,:] * 1.5
            transz = np.array([[transz[0,0], transz[0,1], transz[0,2], transz[0,3],  0.000,   0.000], 
                          [transz[1,0], transz[1,1], transz[1,2], transz[1,3],  0.000,   0.000],
                          [transz[2,0], transz[2,1], transz[2,2], transz[2,3],  0.000,   0.000],
                          [0.04,        0.04,        0.36,        0.40,         0.150,   0.010], 
                          [0.15,        0.05,        0.15,        0.25,         0.250,   0.150],
                          [0.15,        0.05,        0.10,        0.25,         0.100,   0.350]])
            DISTz0 = np.array([DISTz0[0,0], DISTz0[0,1], DISTz0[0,2], DISTz0[0,3],  0, 0])
            
            # Determine productivity distributions for ages
            DISTz_g      = np.zeros(nz,T_life)
            DISTz_g[:,0] = np.reshape(DISTz0, (nz,1))
        
            for age in range(1,T_life):
                DISTz_g[:,age] = np.matmul(np.transpose(transz) , DISTz_g[:,age-1])
        
        
            # Replicate age productivity distributions for population groups
            DISTz = np.tile(DISTz_g[:,:,np.newaxis], [1,1,ng])
            
            # Shift productivity distributions for legal and illegal immigrants
            # towards highest and lowest productivity levels respectively
            for age in range(T_life):
            
                zmean = np.mean(zs[:,age,:], axis = 2)
            
                if not np.array_equal(zmean * DISTz[:,age,g['citizen']] , zmean * DISTz[:,age,g['legal']] * prem_legal ):
                    zlegal   = sum(zmean * DISTz[:,age,g['legal']]) * prem_legal 
                    plegal   = (zmean[nz] - zlegal  ) / (zmean[nz]*(nz-1) - sum(zmean[0:nz-1]))
                    DISTz[:,age,g['legal']] = np.vstack((plegal*np.ones(nz-1,1), 1 - plegal*(nz-1)))

                zillegal = sum(zmean * DISTz[:,age,g['illegal']]) * prem_illegal
                pillegal = (zmean[0]  - zillegal) / (zmean[0] *(nz-1) - sum(zmean[1:nz]))
                DISTz[:,age,g['illegal']] = np.vstack((1 - pillegal*(nz-1), pillegal*np.ones((nz-1,1))))
            
            # Make the larger sized transz grid and replicate the current transz grid.
            transz      = np.vstack((np.hstack((transz, np.zeros[nz])), np.hstack((np.zeros[nz], transz))))
            transitions = np.tile(transz, [1, 1, T_life])
            zs          = np.vstack((zs[:,:,0], zs[:,:,1]))
            DISTz       = np.vstack((DISTz, DISTz)) / 2
                
        elif scenario.LaborShock == 'Kent5':
            
            # LABOR PRODUCTIVITY SHOCKS
            # Life-cycle productivity from Kent's CBO (2017)
            # Transitions matrices comes from Nishiyama and Smetters (2013), computational handbook paper
            # Define: labor_income(age) = e_bar(age) * productivity * wage * labor_supply
            # where: e_bar(age)  : average deterministic income at age age (other work sheet)
            #        productivity: idiosycratic multiplier (other work sheet)
            #        wage        : general wage rate 
            #        labor_supply: labor supply of agent
            
            # Life-cycle productivity
            ebar = np.array([0.2376,	0.3259,	0.409,	0.487,	0.5601,	0.6285,	0.6924,	0.752,	0.8075,	0.859,	0.9067,	0.9507,	0.9912,	1.0284,	1.0624,	1.0933,	1.1213,	1.1464,	1.1688,	1.1886,	1.206,	1.2209,	1.2335,	1.2439,	1.2522,	1.2584,	1.2627,	1.265,	1.2655,	1.2641,	1.261,	1.2561,	1.2496,	1.2414,	1.2316,	1.2201,	1.2071,	1.1924,	1.1762,	1.1583,	1.1389,	1.1177,	1.095,	1.0705,	1.0444,	1.0164,	0.9867,	0.9551,	0.9216,	0.886,	0.8485,	0.8088,	0.7669,	0.7227,	0.6761,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0])
            # Number of shocks
            nz = 5
            # Productivity shocks
            zMarkov = np.array([0.2443564,	0.4943242,	1,	2.022964,	4.092383])
            # Initial distribution
            DISTz0 = np.array([0.0082, 0.2008,	0.5819,	0.2008,	0.0082])/sum([0.0082, 0.2008,	0.5819,	0.2008,	0.0082])
            # Transition matrix
            transz = np.transpose(np.array([[0.8999075,	3.91E-02,	7.78E-07,	2.14E-15,	6.52E-28],
                          [0.1000888,	0.8861447,	5.47E-02,	1.71E-06,	7.58E-15],
                          [3.67E-06,	7.48E-02,	0.8906853,	7.48E-02,	3.67E-06],
                          [0.00E+00,	1.71E-06,	5.47E-02,	0.8861447,	0.1000889],
                          [6.52E-28,	2.14E-15,	7.78E-07,	3.91E-02,	0.8999075]]))
    
            zs = np.zeros((nz,T_life))
            for iz in range(nz):
                for age in range(T_life):
                    # Each shock by age
                    zs[iz,age] = ebar[age] * zMarkov[iz]
                    
                # Re-normalizing transition probabilities to make sure sum = 1
                transz[iz,:] = transz[iz,:]/sum(transz[iz,:])
                
            # Copy transition for every age
            transitions = np.tile(transz[:,:,np.newaxis], [1, 1, T_life])
            
            # Determine productivity distributions for ages
            DISTz_g      = np.zeros((nz,T_life))
            DISTz_g[:,0] = np.reshape(DISTz0, (nz,1))
            
            for age in range(1,T_life):
                DISTz_g[:,age] = np.matmul(np.transpose(transz) * DISTz_g[:,age-2])
            
            # Replicate age productivity distributions for population groups
            DISTz = np.tile(DISTz_g[:,:,np.newaxis], [1,1,ng])
            
            # Shift productivity distributions for legal and illegal immigrants
            # towards highest and lowest productivity levels respectively
            for age in range(T_workMax):

                zmean = zs[:,age]
                
                if not np.array_equal(zmean * DISTz[:,age,g['citizen']] , np.dot(zmean * DISTz[:,age,g['legal']], prem_legal )):
                    zlegal   = sum(zmean * DISTz[:,age,g['legal']-1]) * prem_legal   
                    plegal   = (zmean[nz-1] - zlegal  ) / (zmean[nz-1]*(nz-1) - sum(zmean[0:nz-1]))
                    DISTz[:,age,g['legal']] = np.vstack((np.dot(plegal,np.ones((nz-1,1))), 1 - plegal*(nz-1)))

                    zillegal = sum(zmean * DISTz[:,age,g['illegal']]) * prem_illegal
                    pillegal = (zmean[0]  - zillegal) / (zmean[0] *(nz-1) - sum(zmean[1:nz  ]))
                    DISTz[:,age,g['illegal']] = np.vstack((1 - pillegal*(nz-1), np.dot(pillegal, np.ones((nz-1,1)))))
                    
            # Make sure after-calculations-DISTz sums up to 1 for each age-group bin
            scale       = np.sum(DISTz,axis=0)
            DISTz       = DISTz / scale
                
        elif scenario.LaborShock == 'Kent9':
            # LABOR PRODUCTIVITY SHOCKS
            # Life-cycle productivity from Kent's CBO (2017)
            # Transitions matrices comes from Nishiyama and Smetters (2013), computational handbook paper
            # Define: labor_income(age) = e_bar(age) * productivity * wage * labor_supply
            # where: e_bar(age)  : average deterministic income at age age (other work sheet)
            #        productivity: idiosycratic multiplier (other work sheet)
            #        wage        : general wage rate 
            #        labor_supply: labor supply of agent

            # Life-cycle productivity
            ebar = np.array([0.2376,	0.3259,	0.409,	0.487,	0.5601,	0.6285,	0.6924,	0.752,	0.8075,	0.859,	0.9067,	0.9507,	0.9912,	1.0284,	1.0624	,1.0933,	1.1213,	1.1464,	1.1688,	1.1886,	1.206,	1.2209,	1.2335,	1.2439,	1.2522,	1.2584,	1.2627,	1.265,	1.2655,	1.2641,	1.261,	1.2561,	1.2496,	1.2414,	1.2316,	1.2201,	1.2071,	1.1924,	1.1762,	1.1583,	1.1389,	1.1177,	1.095,	1.0705,	1.0444,	1.0164,	0.9867,	0.9551,	0.9216,	0.886,	0.8485,	0.8088,	0.7669,	0.7227,	0.6761,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0	,0,	0])
            # Number of shocks
            nz = 9
            # Productivity shocks
            zMarkov = np.array([0.05971,	0.12079,	0.24436,	0.49432,	1.00000,	2.02296,	4.09239,	8.27875,	16.74760])
            # Initial distribution
            DISTz0 = np.array([0.002086,	0.018793,	0.090187,	0.230959,	0.315949,	0.230959,	0.090187,	0.018793,	0.002086])/sum([0.002086,	0.018793,	0.090187,	0.230959,	0.315949,	0.230959,	0.090187,	0.018793,	0.002086])
                
            # Transition matrix
            transz = np.transpose(np.array([[0.8316656,	1.87E-02,	1.49E-07,	1.59E-16,	1.84E-29,	0.00E+00,	0.00E+00,	0.00E+00,	0.00E+00],
                          [0.1683187,	0.8501475,	2.73E-02,	3.45E-07,	5.91E-16,	1.11E-28,	0.00E+00,	0.00E+00,	0.00E+00],
                          [1.57E-05,	0.131158,	0.8725756,	3.91E-02,	7.78E-07,	2.14E-15,	6.52E-28,	0.00E+00,	0.00E+00],
                          [0.00E+00,	7.68E-06,	0.1000886,	0.8861446,	5.47E-02,	1.71E-06,	7.58E-15,	3.74E-27,	0.00E+00],
                          [0.00E+00,	0.00E+00,	3.67E-06,	7.48E-02,	0.8906853,	7.48E-02,	3.67E-06,	2.62E-14,	2.09E-26],
                          [0.00E+00,	0.00E+00,	0.00E+00,	1.71E-06,	5.47E-02,	0.8861447,	0.1000885,	7.68E-06,	8.80E-14],
                          [0.00E+00,	0.00E+00,	0.00E+00,	0.00E+00,	7.78E-07,	3.91E-02,	0.8725755,	0.1311582,	1.57E-05],
                          [0.00E+00,	0.00E+00,	0.00E+00,	0.00E+00,	0.00E+00,	3.45E-07,	2.73E-02,	0.8501475,	0.1683187],
                          [0.00E+00,	0.00E+00,	0.00E+00,	0.00E+00,	1.84E-29,	1.59E-16,	1.49E-07,	1.87E-02,	0.8316656]]))
            
            zs = np.zeros((nz,T_life))
            for iz in range(nz):
                for age in range(T_life):
                    # Each shock by age
                    zs[iz,age] = ebar[age] * zMarkov[iz]
                    
                # Re-normalizing transition probabilities to make sure sum = 1
                transz[iz,:] = transz[iz,:]/sum(transz[iz,:])
            
            # Determine productivity distributions for ages
            DISTz_g      = np.zeros((nz,T_life))
            DISTz_g[:,0] = np.reshape(DISTz0, (nz))       
            for age in range(1,T_life):
                DISTz_g[:,age] = np.matmul(np.transpose(transz), DISTz_g[:,age-1])
                
            # Copy transition for every age
            transitions = np.tile(transz[:,:,np.newaxis], [1, 1, T_life])
            
            # Replicate age productivity distributions for population groups
            DISTz = np.tile(DISTz_g[:,:,np.newaxis], [1,1,ng])
            
            # Shift productivity distributions for legal and illegal immigrants
            # towards highest and lowest productivity levels respectively
            for age in range(T_workMax):
               
                if not np.array_equal( zs[:,age] * DISTz[:,age,g['citizen']], zs[:,age] * DISTz[:,age,g['legal']] * prem_legal ):
                    zlegal   = sum(zs[:,age] * DISTz[:,age,g['legal']]) * prem_legal
                    plegal   = (zs[nz,age] - zlegal) / (zs[nz,age]*(nz-1) - sum(zs[0:nz-1,age]))
                    DISTz[:,age,g['legal']] = np.vstack((np.dot(plegal,np.ones((nz-1,1))), 1 - plegal*(nz-1)))    

                    zillegal = sum(zs[:,age] * DISTz[:,age,g['illegal']]) * prem_illegal
                    pillegal = (zs[0,age]  - zillegal) / (zs[0,age] *(nz-1) - sum(zs[1:nz,age]))
                    DISTz[:,age,g['illegal']] = np.vstack((1 - pillegal*(nz-1), np.dot(pillegal*np.ones((nz-1,1)))))
                    
            # Make sure after-calculations-DISTz sums up to 1 for each age-group bin
            scale       = np.sum(DISTz,axis=0)
            DISTz       = DISTz / scale
                
        
        # Checks
        assert np.all(transitions[:] >= 0), 'WARNING! Negative transition probabilities.'
        assert np.all(DISTz      [:] >= 0), 'WARNING! Negative initial distribution of people DISTz.'
        if scenario.prem_legal==1:
            citizen_legal = np.abs(DISTz[:,:,g['citizen']]-DISTz[:,:,g['legal']])
            assert np.all(citizen_legal[:] < 1e-14), ('WARNING! Legal immigrants distribution does not match natives distribution although prem_legal = %f.\n' % scenario.prem_legal )
        
        s = {}
        s['ndem']     = ndem
        s['g']        = g
        s['ng']       = ng
        s['nz']       = ndem*nz
        s['transz']   = transitions
        s['DISTz']    = DISTz
        s['zs']       = zs
        
        return s
        
    ##
    # Output the calculated labor productivity grid as a transitions.csv file
    @staticmethod   
    def export_labor_productivity( scenario ):
        
        # Import data
        params    = ParamGenerator.labinc_discretization(scenario)
        timing    = ParamGenerator.timing(scenario)
        filename1 = 'labor_productivity.csv'
        filename2 = 'transitions.csv'
        filename3 = 'initial_DIST.csv'
        
        # Export labor productivity grids and transition probabilities
        
        colNames_vals = ['Year', 'Age']
        colNames_trans = ['Year', 'Age']
        for i in range(1, params['nz']+1):
            colNames_vals.append('Value%u' % i)
            for j in range(1, params['nz']+1):
                colNames_trans.append('Transition_%u_%u' % (i, j))

        #TBD pre-allocate space instead of appending on each loop
        V = np.zeros((0,len(colNames_vals)))
        T = np.zeros((0,len(colNames_trans)))
               
        # TEMP: No time variation, so use steady year and first transition
        #with warnings.catch_warnings():
        #    warnings.simplefilter("ignore")
            
        for age in range(timing['realage_entry'], timing['realage_entry'] + timing['T_life']):
            a = age - timing['realage_entry']
            vrow = np.array([timing['TransitionFirstYear'] - 1, age])
            trow = np.array([timing['TransitionFirstYear'] - 1, age])
            for z in range(params['nz']):
                vrow = np.append(vrow, params['zs'][z, a])
                trow = np.append(trow, params['transz'][z, 0:params['nz'], a])
            T = np.vstack((T, trow))
            V = np.vstack((V, vrow))
            
        Trows = T.shape[0]
            
        T = pd.DataFrame(T, columns = colNames_trans)
        V = pd.DataFrame(V, columns = colNames_vals)
            
        # TEMP: No time variation, so use steady year and first transition
        Tout = np.copy(T.values)
        Vout = np.copy(V.values)
            
        T['Year'] = (timing['TransitionFirstYear']) * np.ones(Trows)
        V['Year'] = (timing['TransitionFirstYear']) * np.ones(Trows)
        
        Tout = np.vstack((Tout, T.values))
        Vout = np.vstack((Vout, V.values))
           
        T['Year'] = (timing['TransitionFirstYear'] + 1) * np.ones(Trows)
        V['Year'] = (timing['TransitionFirstYear'] + 1) * np.ones(Trows)
        
        Tout = np.vstack((Tout, T.values))
        Vout = np.vstack((Vout, V.values))
        
        Tout = pd.DataFrame(Tout, columns = colNames_trans)
        Vout = pd.DataFrame(Vout, columns = colNames_vals)
        
        Vout.to_csv(filename1)
        Tout.to_csv(filename2)
        
        # Export initial household distribution wrt labor productivity
        colNames_DIST  = np.array(['Year', 'Age', 'Group'])
        for iz in range(1, params['nz']+1):
            colNames_DIST = np.append(colNames_DIST, 'Mass_%u' % iz)
        
        D = np.zeros((0,len(colNames_DIST)))
        
        #with warnings.catch_warnings():
        #    warnings.simplefilter("ignore")
        
        for age in range(timing['realage_entry'], timing['realage_entry'] + timing['T_life']):
            model_age     = age - timing['realage_entry']
            for ig in range(params['ng']):
                row = np.array([timing['TransitionFirstYear'] - 1, age, ig])
                row = np.append(row, params['DISTz'][0:params['nz'], model_age, ig])
                D = np.vstack((D,row))
        
        Drows = D.shape[0]
        
        D = pd.DataFrame(D, columns = colNames_DIST)
        
        # TEMP: No time variation, so use steady year and first transition
        Dout = np.copy(D.values)
        D['Year'] = (timing['TransitionFirstYear']) * np.ones(Drows)
        Dout = np.vstack((Dout, D.values))
        D['Year'] = (timing['TransitionFirstYear'] + 1) * np.ones(Drows)
        Dout = np.vstack((Dout, D.values))

        Dout = pd.DataFrame(Dout, columns = colNames_DIST)
        Dout.to_csv(filename3)           
        
    ## INDIVIDUAL TAXES
    # 
    # Generate tax policy parameters according to predefined plans.
     
    def taxIndividual(scenario):
        
        timing          = ParamGenerator.timing(scenario)
        T_model         = timing['T_model']
        first_year      = timing['yearFirst']
        last_year       = timing['yearLast']
        
        pathFinder      = PathFinder(scenario)
        
        # Get the capital and tax treatment allocation params.
        #    Rem: These are time-varying, so read results are vectors.
        file     = pathFinder.getTaxCalculatorInputPath( 'CapitalTaxes' )
        tax_vars = InputReader.read_series(file, 'Year', first_year, last_year)    
        # Portion of corporate income taxed at preferred rates
        #   TBD: Fix this when John R. gives new inputs
        s = {}
        s['sharePreferredCorp'] = tax_vars['sharePreferredCorp']
        s['shareOrdinaryCorp']  = tax_vars['shareOrdinaryCorp']
        s['shareOrdinaryPass']  = tax_vars['shareOrdinaryPass']       
        
        # Capital gains tax == zero for now
        s['rateCapGain'] = np.zeros(T_model)
        
        # ORDINARY RATES
        # Read Ordinary Rates & Brackets
        file     = pathFinder.getTaxCalculatorInputPath( 'OrdinaryRates' )
        (brackets, rates, _) = InputReader.read_brackets_rates_indices(file, first_year, T_model)        
        # TBD: This should be done in ModelSolver as for SocialSecurity
        # Calculate cumulative tax burdens along brackets dimension
        burdens         = np.cumsum(np.diff(brackets, axis = 1) * rates[:, 0:-1], axis = 1) 
        burdens         = np.hstack((np.zeros((len(brackets), 1)), burdens))  # rem: first burden is zero       
        # Convert US dollar amounts into modelunit_dollars
        s['burdens']       = burdens  * scenario.modelunit_dollar    # Cumulative tax burden
        s['brackets']      = brackets * scenario.modelunit_dollar    # PIT tax brackets, rem: first one is zero
        s['rates']         = rates                                   # Rate for above each bracket threshold
        
        # Social Security income deductions
        file          = pathFinder.getOASIcalculatorInputPath('BenefitTaxes')
        series        = InputReader.read_series(file, 'Year', first_year , last_year)
        s['sstaxcredit'] = 1 - series['BenefitTax']
        
        
        # CONSUMPTION TAX RATES
        # Read Ordinary Rates & Brackets
        file     = pathFinder.getTaxCalculatorInputPath('ConsRates')
        (brackets, rates, _) = InputReader.read_brackets_rates_indices(file, first_year, T_model)        
        # Calculate cumulative tax burdens along brackets dimension
        burdens         = np.cumsum(np.diff(brackets, axis = 1) * rates[:, 0:-1], axis = 1) 
        burdens         = np.hstack((np.zeros((len(brackets), 1)), burdens))  # rem: first burden is zero       
        # Convert US dollar amounts into modelunit_dollars
        s['consburdens']       = burdens * scenario.modelunit_dollar     # Cumulative tax burden
        s['consbrackets']      = brackets * scenario.modelunit_dollar    # Consumption tax brackets, rem: first one is zero
        s['consrates']         = rates                                   # Rate for above each bracket threshold
        
        
        # PREFERRED RATES
        # Read Preferred Rates & Brackets
        file     = pathFinder.getTaxCalculatorInputPath('PreferredRates')
        (brackets, rates, _) = InputReader.read_brackets_rates_indices(file, first_year, T_model)       
        # TBD: This should be done in ModelSolver as for SocialSecurity
        # Calculate cumulative tax burdens along brackets dimension
        burdens         = np.cumsum(np.diff(brackets, axis = 1) * rates[:, 0:-1], axis = 1) 
        burdens         = np.hstack((np.zeros((len(brackets), 1)), burdens))  # rem: first burden is zero
        
        # Convert US dollar amounts into modelunit_dollars
        s['prefburdens']   = burdens  * scenario.modelunit_dollar    # Cumulative tax burden
        s['prefbrackets']  = brackets * scenario.modelunit_dollar    # Preferred Rate tax brackets, rem: first one is zero
        s['prefrates']     = rates                                   # Rate for above each bracket threshold
        
        # Warn if parameters are outside expectations
        if np.any(s['sharePreferredCorp'] < 0) or np.any(s['sharePreferredCorp'] > 1):
            print( 'WARNING! ParamGenerator.taxIndividual.sharePreferredCorp outside expecations.\n' )
        
        if np.any(s['shareOrdinaryCorp'] < 0) or np.any(s['shareOrdinaryCorp'] > 1):
            print( 'WARNING! ParamGenerator.taxIndividual.shareOrdinaryCorp outside expecations.\n' )
            
        if np.any(s['shareOrdinaryPass'] < 0) or np.any(s['shareOrdinaryPass'] > 1):
            print( 'WARNING! ParamGenerator.taxIndividual.shareOrdinaryPass outside expecations.\n' )
        
        ## TBD: Enlarge the space of error checks
        
        return s

    ## BUSINESS TAXES
    # 
    # Generate tax policy parameters according to predefined plans.
     
    def taxBusiness( scenario ):
        
        timing          = ParamGenerator.timing(scenario)
        T_model         = timing['T_model']
        first_year      = timing['yearFirst']
        last_year       = timing['yearLast']
        
        pathFinder = PathFinder(scenario)
        
        # Get the capital and tax treatment allocation params. 
        #    Rem: These are time-varying, so read results are vectors.
        file     = pathFinder.getTaxCalculatorInputPath( 'CapitalTaxes' )
        tax_vars = InputReader.read_series(file, 'Year', first_year, last_year)
        s = {}
        
        # Pass along all capital tax parameters 
        for f in tax_vars.keys():
            s[f] = tax_vars[f]
        
        # Income shares sum to 1
        s['shareIncomePass'] = 1 - s['shareIncomeCorp']
        # Tax rate on total foreigner's business income
        s['rateForeignBusinessIncome'] = (s['shareIncomeCorp'] * s['rateForeignerCorpIncome']
                                    + s['shareIncomePass'] * s['rateForeignerPassIncome'])
        
        # Get the household ordinary tax treatment allocation params to use as pass-through rate. 
        #    Rem: These are time-varying, so read results are vectors.
        file     = pathFinder.getTaxCalculatorInputPath( 'OrdinaryRates' )
        (_, pass_rates, _) = InputReader.read_brackets_rates_indices(file, first_year, T_model)        
        s['ratePass']      = pass_rates[:,-1]
        
        # Warn if parameters are outside expectations
        if np.any(s['rateExpensingCorp'] < 0) or np.any(s['rateExpensingCorp'] > 1):
            print( 'WARNING! ParamGenerator.taxBusiness.rateExpensingCorp outside expectations.\n' )
        
        if np.any(s['rateExpensingPass'] < 0) or np.any(s['rateExpensingPass'] > 1):
            print( 'WARNING! ParamGenerator.taxBusiness.rateExpensingPass outside expectations.\n' )
        
        if np.any(s['rateCorpStatutory'] < 0) or np.any(s['rateCorpStatutory'] > 1):
            print( 'WARNING! ParamGenerator.taxBusiness.rateCorpStatutory outside expectations.\n' )
        
        # TBD: Enlarge the space of error checks
        
        return s
    
    ## DEMOGRAPHICS
    #    Includes:
    #          survival probabilities
    #        , immigrant age distribution
    #        , birth rate
    #        , legal immigration rate
    #        , illegal immigration rate
    def demographics( scenario ): 
         
        pathFinder = PathFinder(scenario)
        s = {}
        
        # Initializing the timing variables and the grids 
        timing    = ParamGenerator.timing( scenario )

        file      = pathFinder.getMicrosimInputPath( 'SurvivalProbability' )
        series    = InputReader.read_series( file, 'Age', 1 + timing['realage_entry'] )
        survival  = series['SurvivalProbability']
        if len(survival) != timing['T_life'] :
            raise Exception('timing: Survival probabilities must exist for every age.')
        
        file      = pathFinder.getMicrosimInputPath( 'ImmigrantAgeDistribution' )
        series    = InputReader.read_series( file, 'Age', 1 + timing['realage_entry'] )
        imm_age   = series['ImmigrantAgeDistribution']
        if len(imm_age) != timing['T_life'] :
            raise Exception('timing: Immigrant age distributions must exist for every age.')
        
        s['surv']             = np.tile(np.reshape(survival, (1, timing['T_life'])), [timing['T_model'], 1])
        s['birth_rate']       = 0.018923919 * np.ones(timing['T_model'])                      # Birth rate by year
        s['legal_rate_age']   = 0.002371966 * np.transpose(np.matmul(imm_age[:,np.newaxis],np.ones((1,timing['T_model']))))            # Net entry rate of legal migrants by age & year
        s['illegal_rate_age'] = 0.000836294 * np.transpose(np.matmul(imm_age[:,np.newaxis],np.ones((1,timing['T_model']))))            # Net entry rate of illegal migrants by age & year 
        
        return s
   
    ## DEMOGRAPHICS (NEW VERSION)
    #     Includes:
    #          survival probabilities
    #        , immigrant age distribution
    #        , birth rate
    #        , legal immigration rate
    #        , illegal immigration rate
    #        , newDISTz
    def demographicsNew( scenario ): 
        
        pathFinder = PathFinder(scenario) 
        s = {}
        
        # Initializing the timing variables and the grids 
        timing     = ParamGenerator.timing( scenario )
        modelYears = timing['T_model'] + 1
        grids      = ParamGenerator.grids(  scenario )
        firstYear  = timing['TransitionFirstYear']
        if scenario.isSteady():
            firstYear = timing['TransitionFirstYear'] - 1
        
        # nonMJShocks indicates the indexes for the non-Michael Jordan shocks
        # productivityBounds are the bounds of the high and low productivity
        # individuals. Kent9 and Kent5 do not have these groups, so their
        # vectors are single elements. There are high and low productivity
        # groups for the MJ10 and MJ12 specifications, so the vectors have two
        # elements in them.
        # Elements 1-5 and 1-6 are the lower productivity groups
        # for MJ10 and MJ12 groups, so the upper bounds for the lower group
        # are 5 and 6. The upper group have upper indices of 10 and 12.
        # ImmigrantToCitizenRatio is a vector of the weights of the
        # immigrants relative to baseline.  So a vector of ones leads to
        # the distribution of the productivity of legal immigrants that is
        # the same in the policy as in the baseline. Change the weights to
        # non-zero values to change the relative weights of the
        # productivity levels compared to baseline.
        # TBD: import immigrantToCitizenRatio from file according to policy
        
        if scenario.LaborShock == 'Kent9':
            immigrantToCitizenRatio = [1, 1, 1, 1, 1, 1, 1, 1, 1]
        elif scenario.LaborShock == 'Kent5':
            immigrantToCitizenRatio = [1, 1, 1, 1, 1]
        elif scenario.LaborShock == 'MichaelJordan10':
            immigrantToCitizenRatio = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        elif scenario.LaborShock == 'MichaelJordan12':
            immigrantToCitizenRatio = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

        # Set initial distribution of individuals by age-productivity-group
        # Note: at some point we could make DISTz time-varying.
        newDISTz = grids['DISTz']
        
        # Truncate the distribution of productivity for illegal immigrants
        # to match a moment from DATA: the target value (90% right now)
        # for ratio of illegal/native productivity of non-MJ income earners.
        newDISTz[:,:,grids['g']['illegal']] = ParamGenerator.truncateDISTz(scenario.prem_illegal, newDISTz[:,:,grids['g']['citizen']], timing['T_life'], np.squeeze(grids['zs'][0,:,:]) )       

        # Check migration policy parameter dimension and requirements
        assert len(immigrantToCitizenRatio) == grids['nz'], 'Productivity shock dimension NOT equal to that of immigrantToCitizenRatio.'
        assert immigrantToCitizenRatio[0] == 1.0, 'The first value of immigrantToCitizenRatio needs to be one.'
        assert min(immigrantToCitizenRatio) >= 0, 'Elements of immigrantToCitizenRatio to adjust the productivity distribution must be positive.'
        # Modify the distribution of legal immigrants to accomodate an 
        # immigration policy that targets immigrants' of particular
        # productivity levels compared to natives
        for age in range(timing['T_life']):
            newDISTz[:,age,grids['g']['legal']] = newDISTz[:,age,grids['g']['legal']] * immigrantToCitizenRatio
            newDISTz[:,age,grids['g']['legal']] = newDISTz[:,age,grids['g']['legal']] / sum(newDISTz[:,age,grids['g']['legal']])            
        
        # Saving newDISTz
        s['newDISTz'] = newDISTz

        file       = pathFinder.getDemographicsInputPath( 'rates' )
                     
        # Loading in the microsim module with demographics and immigration
        # data
        
        # The numbers in the microsim model for "status" do not match up
        # with our grid mapping.  From the Microsim model:
        # 0: Naturalized citizen
        # 1: Lawful noncitizen
        # 2: Unauthorized immigrant
        # 3: Native-born citizen
        
        # Therefore, we create a map from the microsim to the OLG.
        
        microSimStatus = {}
        microSimStatus['naturalized']      = 0
        microSimStatus['legal']            = 1
        microSimStatus['illegal']          = 2
        microSimStatus['native']           = 3 
        
        statusMap = np.array([['naturalized', 'citizen'],   # Naturalized citizens in Microsim are citizens in the OLG
                     ['legal',       'legal'],     # Lawful noncitizen is "legal" in OLG
                     ['illegal',     'illegal'],   # Unathorized noncitizen is "illegal" in OLG
                     ['native',      'citizen']])   # Native born citizen is "citizen" in the OLG
        
        demographics = InputReader.read_demographics(file, firstYear, modelYears, timing['realage_entry'], timing['T_life'], grids['g'], microSimStatus, statusMap, scenario)
        
        # Note: starting year is 2018 in read_demographics because input file still lacks 2017. Replace by timing.yearFirst.
        # Each field in demographics is a (T_model x T_life) matrix
        
        # Notes on the demographics interface.  It's tough.
        # "Population" is the period population after immigrants enter
        # "Emigrated" is the number of people leaving the U.S., *before*
        # deaths are applied to the population.
        # "Died" are the number of people who remained in the country who
        # died.
        # "Immigrated" are the number of people added to the prior-year
        # "Born" is denoted people entering the simualtion at age 21.  This
        # *includes* immigrants who immigrated before age 21.
        # "Population" and "Born" variables to get this year's population.
        
        # Here's the flow chart for immigration
        
        # Population (t-1)
        # subtract Emigrated (t-1)
        # subtract Died (t-1)
        # add Born (t)
        # add Immigrated (t)
        # equals Population (t)
        
        # Population per year
        population           = np.sum(demographics['citizen']['Population'] + demographics['legal']['Population'] + demographics['illegal']['Population'], axis=1)
        
        # To calculate survival probabilities, we need to know how many
        # people died of the number of people who did not return to their
        # home countries.  Therefore we compute a variable
        # "local_population_age".
        local_population_age = ((demographics['citizen']['Population'] + demographics['legal']['Population'] + demographics['illegal']['Population']) - 
                               (demographics['citizen']['EmigratedThisYear'] + demographics['legal']['EmigratedThisYear'] + demographics['illegal']['EmigratedThisYear']))
        
        # Time-varying survival probabilities by age
        # The survival probabiilty has to be computed on the population
        # that's still here ("population" is before emigration, "Emigrated"
        # is after emigration, so it's deaths among people who are left).    
        # People at the end of life survive with 0 probability.
        surv                    = 1 - (demographics['citizen']['DiedThisYear'] + demographics['legal']['DiedThisYear'] + demographics['illegal']['DiedThisYear']) / local_population_age
        surv[:,timing['T_life']-1]   = 0
        
        # Time-varying entry rates by age
        # Note: Immigrants that arrived before age 21 are accounted for as new borns
        # Emigration is at the end of the previous period, so it's
        # immigration in the current period minus emigration in the
        # previous period;
        # Note: in period ONE, we don't have the emigrants.  We only have
        # the people who immigrate as age 21.  
        # Note: we set the denominator a minimum of 1 if there are no people;
        # obviously nobody emigrates so, we're effectively setting the
        # value to NaN when the population is 0.
        s['surv']                     = surv
        s['citizen'] = {}
        s['legal'] = {}
        s['illegal'] = {}
        s['citizen']['emigrationRate']   = np.zeros(demographics['legal']['EmigratedThisYear'].shape)
        s['legal']['emigrationRate']     = demographics['legal']['EmigratedThisYear'] / np.maximum(np.ones(demographics['legal']['Population'].shape),demographics['legal']['Population'])
        s['illegal']['emigrationRate']   = demographics['illegal']['EmigratedThisYear']  / np.maximum(np.ones(demographics['illegal']['Population'].shape),demographics['illegal']['Population'])

        # We use this to compute the rates for immigration and births. We
        # need birth rates that are a function of the previous period's
        # population.  So we compute birth and immigration rates as the
        # number of births and the number of immigrants divided by *last
        # year's* population.  These two lines construct the appropriate
        # denominator so we can do element-by-element division.
        
        population = population[:, np.newaxis] #turn population into Matlab-like column vector so that following computations behave the same
        
        baselinePop             = np.tile(population, [1,timing['T_life']])
        baselinePop[1:modelYears, :]    = baselinePop[0:modelYears-1,:]
        
        # This is the birth rates and immigration rates, normalized to the
        # same period populations. (These are "multiplicative" factors")
        s['citizen']['birthRate']          = demographics['citizen']['BornThisYear'] / baselinePop
        s['legal']['birthRate']            = demographics['legal']['BornThisYear'] / baselinePop
        s['illegal']['birthRate']          = demographics['illegal']['BornThisYear'] / baselinePop
        s['citizen']['immigrationRate']    = np.zeros(demographics['legal']['ImmigratedThisYear'].shape)
        s['legal']['immigrationRate']      = demographics['legal']['ImmigratedThisYear'] / baselinePop
        s['illegal']['immigrationRate']    = demographics['illegal']['ImmigratedThisYear'] / baselinePop
        
        # Creating the populations normalized to their first period sizes
        firstPeriodPop       = np.sum(population[0,:])
        s['citizen']['initPop']    = demographics['citizen']['Population'][0,:] / firstPeriodPop
        s['legal']['initPop']      = demographics['legal']['Population'][0,:]   / firstPeriodPop
        s['illegal']['initPop']    = demographics['illegal']['Population'][0,:] / firstPeriodPop
    
        return s
    
    ## Truncate distribution to match moment from data
    #     Finds the percentage to truncate a group distribution so that its
    #     weighted average compensation hits a target proportion of another
    #     (typically illegals earn some fraction of natives, so we're
    #     truncating the distribution to hit that fraction), but *only* for
    #     the non-MJ shocks.  This percentage of the top of the
    #     distribution that is truncated is age-varying.
    #
    def truncateDISTz(premium, baseDISTz, T_life, zs):

        # Inputs: nonMJShocks: indexes for the non-Michael Jordan shocks
        #         productivityBounds: bounds for high and low productivities
        #         premium: average group productivity / average base population productivity
        #         baseDISTz: base pop distribution by age-productivity, e.g., natives
        #         T_life: the maximum length of life
        #         zs: productivity shock values
        # Output: groupDISTz: truncated distribution that generates an average
        #                     group wage = premium * base population wages

        groupDISTz     = baseDISTz.copy()           # Initializing the output
        dampingFactor  = 0.05                # If the algorithm is not converging to a solution, try smaller values of this to make sure it's not "jumping" over the solution.
        shareTruncated = 1                  # One minus this number is what is removed from the top of the distribution; this is the initial guess
        
        for age in range(T_life):
            
            # Checking to make sure that this is a non-zero productivity (non-retirement)
            if (sum(np.squeeze(zs[age,:]) * np.squeeze(baseDISTz[:,age])) > .0001):
                
                for iter in range(1000):
                    
                    productivityArray = np.sort(np.squeeze(zs[age,:]))
                    sortIndex = np.argsort(np.squeeze(zs[age,:]))
                    
                    # Initialize the truncated distribution for one age
                    truncAgeDISTz = groupDISTz[sortIndex,age]

                    # This loop goes through each grid point in that productivity
                    # group and reallocates the distribution.
                   
                    totProb       = 0
                    upperBound    = max(productivityArray.shape)
                    prodThreshold = sum(truncAgeDISTz[0:upperBound]) * shareTruncated
                    for jz in range(upperBound):
                        # Walk through the distribution from the bottom, making sure
                        # that we're not at the threshold yet
                        if totProb < prodThreshold:
                            totProb = totProb + truncAgeDISTz[jz]
                            # If this puts the probability over the threshold, stop
                            # and only take away enough from the top grid point so
                            # that it hits, but does not exceed the threshold
                            if totProb > prodThreshold:
                                truncAgeDISTz[jz] = truncAgeDISTz[jz] - (totProb - prodThreshold)
                        else:
                            truncAgeDISTz[jz] = 0
                    
                    # Reallocate the truncated part proportionally to the remaining
                    # part of the distribution.
                    truncAgeDISTz = truncAgeDISTz / shareTruncated
                    
                    # This is the inverse sort: it's taking the density for
                    # the sorted productivity and unwinding back to the
                    # original way that the productivity shocks were
                    # ordered.
                    truncAgeDISTz[sortIndex] = truncAgeDISTz

                    # Update the share to be truncated
                    groupRatio     = (sum(np.squeeze(zs[age,:]) * np.squeeze(truncAgeDISTz)) / 
                                      sum(np.squeeze(zs[age,:]) * np.squeeze(baseDISTz[:,age])))
                    shareTruncated = min(max(shareTruncated + dampingFactor * (premium - groupRatio),0),1)
                
                groupDISTz[:,age] = truncAgeDISTz
                shareTruncated  = 1
        
        # Check to make sure that the ratio is correct and throw a warning,
        # skipping those with NaN (the denominator is 0 b/c there is no
        # productivity for retired people)
        with np.errstate(divide='ignore',invalid='ignore'):
            groupRatioCohortError = max(np.abs(premium - np.sum(zs * np.transpose(groupDISTz),axis=1) / np.sum(zs * np.transpose(baseDISTz),axis=1)))
            
            if np.isnan(groupRatioCohortError):
                groupRatioCohortError = 0
            if np.abs(groupRatioCohortError) > 0.0001:
                print( 'WARNING! Did not hit the target income ratio for a citizenship status group.\n')
    
        return groupDISTz
    
    ## OUT OF MODEL ADJUSTMENTS
    #     Includes:
    #         TFP
    def outofmodel(scenario, budget): 
    
        timing      = ParamGenerator.timing(scenario)
        first_year  = timing['yearFirst']
        last_year   = timing['yearLast']
        
        pathFinder  = PathFinder(scenario)
        s = {}

        # Import the out-of-model parameters and variables
        file        = pathFinder.getOutOfModelInputPath( 'outofmodel' )
        projections_series      = InputReader.read_series(file, 'Year', first_year, last_year)
        
        # The interface takes TFP in ratios.  So if the values are 1, then
        # TFP is unchanged.  The infraSpending and lumpSumTaxes components
        # take values that are in per-capita nominal dollars. Here we
        # deflate it and convert it into model units.
        s['TFP']        = projections_series['TFP']
        s['infraSpending'] = projections_series['infraSpending'] * scenario.modelunit_dollar / budget['deflator']
        s['lumpSumTaxes']  = projections_series['lumpSumTax'] * scenario.modelunit_dollar / budget['deflator']
           
        return s
    
    ## PRODUCTION
    #     Includes:
    #        , depreciation
    #        , capital share
    def production(scenario):
        
        s = {}
        
        # TBD: These are hard coded for now, until we resolve sourcing
        s['alpha']         = 0.34                 # Capital share of output
        s['risk_premium']  = 0
        s['depreciation']  = 0.056                # Actual depreciation rate
        if scenario.IsLowReturn :
            s['risk_premium']  = 0.08 - s['depreciation']  # "Depreciation rate" to generate r=risk-free rate         ;
        
        s['capitalAdjustmentCost'] = scenario.CapitalAdjustmentCost
        s['allowBusinessDebt']     = scenario.AllowBusinessDebt
        s['leverageSensitivity']   = scenario.LeverageSensitivity
        
        # Find initial year business debt / capital leverage ratios
        #   TEMP: This should come from interface
        #         For now, this is from Barro-Furman 2018
        s['leverageCorp0']         = 0.32
        s['leveragePass0']         = 0.32

        return s
        
    ## SOCIAL SECURITY
    #
    def social_security( scenario ):
        
        
        timing                  = ParamGenerator.timing(scenario)
        T_model                 = timing['T_model']
        T_life                  = timing['T_life']
        first_year              = timing['yearFirst']
        last_year               = timing['yearLast']
        nstartyears             = len(timing['startyears'])
        realage_entry           = timing['realage_entry']
        
        pathFinder = PathFinder(scenario)
        s = {}
        
        # Get T_works (retirement ages)
        nrafile = pathFinder.getOASIcalculatorInputPath( 'retirementAges' )
        T_works        = np.zeros(nstartyears)
        
        if scenario.isSteady() :
            survivalprob = ParamGenerator.demographics(scenario)['surv'][0]
            series       = InputReader.read_series( nrafile, 'birthYear', first_year - (T_life + realage_entry) )
            T_works      = np.around(series['NRA'])
            mass         = np.ones(T_life)
            for i in range(1,T_life):
                mass[i] = mass[i-1]*survivalprob[i-1]
            T_works      = np.around(np.sum((mass * T_works[0:T_life])/np.sum(mass))) - realage_entry
            T_works = np.array([T_works])
        else:
            series       = InputReader.read_series( nrafile, 'birthYear', first_year - (T_life + realage_entry), last_year )
            T_works      = np.around(series['NRA'])
            T_works      = T_works[0:nstartyears] - realage_entry
        
        s['T_works']           = T_works
        retire_years        = np.zeros(nstartyears)
        for i in range(nstartyears):
            retire_years[i]  =  (i + 1 - T_life) + T_works[i]
        
        s['retire_years']      = retire_years
        #  TAXATION
        
        # Read tax brackets and rates on payroll 
        #   Pad if file years do not go to T_model, truncate if too long
        #   Calculate cumulative liability to speed up calculation 
        #   Convert from US dollars to modelunit dollars
        file = pathFinder.getOASIcalculatorInputPath( 'TaxParameters' )
        (brackets, rates, indices) = InputReader.read_brackets_rates_indices(file, first_year, T_model)
        
        if indices.shape[1] != brackets.shape[1] :
            raise Exception('SSTaxBrackets and BracketsIndexes must have same number of brackets.')
    
        s['taxbrackets']   = brackets * scenario.modelunit_dollar     # Payroll tax brackets, rem: first one is zero
        s['taxrates']      = rates                                       # Rate for above each bracket threshold
        s['taxindices']    = indices                                     # Type of index to use for the bracket change
        
        # BENEFITS
        
        # Get range of income which is credited toward benefit calculation
        file       = pathFinder.getOASIcalculatorInputPath( 'BenefitParameters' )
        (brackets,_,_)   = InputReader.read_brackets_rates_indices(file, first_year, T_model)
        
        s['ssincmins'] = brackets[:, 0] * scenario.modelunit_dollar
        s['ssincmaxs'] = brackets[:, 1] * scenario.modelunit_dollar
        
        # Fetch initial benefits for each cohort 
        #   REM: Benefits are per month in US dollars 
        #        in year = first_transition_year - 1
        first_birthyear = first_year - (T_life + realage_entry)
        last_birthyear  = first_birthyear + nstartyears - 1
        
        file    = pathFinder.getOASIcalculatorInputPath( 'PIAParameters' )
        cohorts = InputReader.read_cohort_brackets_rates_indices(file, first_year, T_model, first_birthyear, last_birthyear)
        
        # Convert birthyears into model startyears
        # Convert monthly brackets into model units and yearly
        s['benefitParamsByCohort'] = np.full((last_birthyear+1-first_birthyear), {})
        for bYear in range(first_birthyear,last_birthyear+1):
           birthYearField = 'b%u' % bYear 
           i              = bYear - first_birthyear   # cohort number
           brackets       = cohorts[birthYearField]['brackets'] * 12 * scenario.modelunit_dollar
           rates          = cohorts[birthYearField]['rates']
           s['benefitParamsByCohort'][i] = {'brackets': brackets, 'rates': rates}
        
        
        return s
    
    
    ## BUDGET AND INTEREST RATES
    #
    def budget( scenario ):
        
        s = {}
        timing                  = ParamGenerator.timing( scenario )
        T_model                 = timing['T_model']
        T_life                  = timing['T_life']
        first_year              = timing['yearFirst']
        last_year               = timing['yearLast']
        
        isSteadyEconomy         = scenario.isSteady()
        
        pathFinder = PathFinder(scenario)
                
        projections_file   = pathFinder.getProjectionsInputPath('Projections')
        projections_series = InputReader.read_series(projections_file, 'Year', first_year, last_year)

        deepHistory_file   = pathFinder.getProjectionsInputPath('DeepHistory')
        history_series     = InputReader.read_series(deepHistory_file, 'Year', first_year - T_life + 1, last_year)
        
        taxcalculator_file      = pathFinder.getTaxCalculatorInputPath( 'Aggregates' )
        taxcalculator_series    = InputReader.read_series(taxcalculator_file, 'Year', first_year, last_year )
        
        oasicalculator_file     = pathFinder.getOASIcalculatorInputPath( 'aggregates' )
        oasicalculator_series   = InputReader.read_series(oasicalculator_file, 'Year', first_year, last_year)
        
        f_revenues = lambda t,o: (
             t['RevenuesIndividualIncomeTax']       
            + t['RevenuesPayrollTaxExSocialSecurity'] 
            + t['RevenuesCorporateIncomeTax']         
            + t['RevenuesEstateAndGiftTaxes']         
            + t['RevenuesExciseTaxes']               
            + t['RevenuesCustomsDuties']             
            + t['RevenuesMiscellaneousReceipts']   
            + t['RevenuesPayrollTaxSocialSecurity'])   

        revenues = f_revenues(taxcalculator_series, oasicalculator_series)
        
        f_noninterestspending = lambda p, t, o: (
            p['OutlaysDiscretionary']       
            + p['OutlaysMedicare']             
            + p['OutlaysMedicaid']            
            + p['OutlaysFederalRetirement']    
            + p['OutlaysVeteransPrograms']     
            + p['OutlaysOtherPrograms']       
            + p['OutlaysOffsettingReceipts']   
            + t['OutlaysIncomeSecurity']       
            + o['OutlaysSocialSecurity']  )
        
        noninterestspending = f_noninterestspending(projections_series, taxcalculator_series, oasicalculator_series)
        
        # Only calculate debttoout for steady
        if isSteadyEconomy :
            projections_past_series     = InputReader.read_series(projections_file   , 'Year', first_year, first_year)
            taxcalculator_past_series   = InputReader.read_series(taxcalculator_file , 'Year', first_year, first_year)
            oasicalculator_past_series  = InputReader.read_series(oasicalculator_file, 'Year', first_year, first_year)

            past_noninterestspending    = f_noninterestspending(projections_past_series, taxcalculator_past_series, oasicalculator_past_series)
            past_revenues               = f_revenues(taxcalculator_past_series, oasicalculator_past_series)

            # Calculate debt to get DEBT/GDP for first_year
            deficit_nis     = past_revenues - past_noninterestspending
            debt            = np.zeros(deficit_nis.shape)
            debt[0]         = projections_series['DebtHeldByThePublic'][0]
            for i in range(1,len(deficit_nis)):
                debt[i] = debt[i-1] * (1 + projections_past_series['AverageInterestRateOnDebt'][i]) - deficit_nis[i]
            
            s['debttoout'] = debt[-1] / projections_series['GDP_FY'][0]
          
        # Rates
        #    For interest rate in steady-state, we use avg. rate across all data
        #    NOTE: EffectiveInterestRateOnDebt is in NOMINAL terms and we
        #    deflate by GDPDeflator. We therefore need N+1 GDPDeflators to
        #    match N interest rates.
        if isSteadyEconomy:
            GDPdeflator_series = InputReader.read_series(projections_file, 'Year', first_year)
        else:
            GDPdeflator_series = InputReader.read_series(projections_file, 'Year', first_year, last_year + 1)
        
        # We need to load in history for the deflator series to get a value
        # for the real interest rate on bonds in the steady state.
        
        GDPdeflator_series_temp = InputReader.read_series(projections_file, 'Year', first_year-1, first_year)
        steadyStatePriceGrowth = GDPdeflator_series_temp['GDPDeflator'][1]/GDPdeflator_series_temp['GDPDeflator'][0]
        
        # Loading in the treasury rates from the projections interface.
        
        treas1yr        = projections_series['TreasuryYield1yr']/100
        treas2yr        = projections_series['TreasuryYield2yr']/100;
        treas3yr        = projections_series['TreasuryYield3yr']/100;
        treas5yr        = projections_series['TreasuryYield5yr']/100;
        treas7yr        = projections_series['TreasuryYield7yr']/100;
        treas10yr       = projections_series['TreasuryYield10yr']/100;
        treas30yr       = projections_series['TreasuryYield30yr']/100;
        
        gdpPriceIndex0  = GDPdeflator_series['GDPDeflator'][0]  
        gdpPriceIndex   = GDPdeflator_series['GDPDeflator'][1:]
        num_rates       = len(gdpPriceIndex)
        interest_rate   = GDPdeflator_series['AverageInterestRateOnDebt'][0:num_rates]
        
        deflator_rate           = np.zeros(num_rates)
        deflator_rate[0]        = gdpPriceIndex[0] / gdpPriceIndex0
        for i in range(1,num_rates):
            deflator_rate[i]    = gdpPriceIndex[i]/gdpPriceIndex[i-1]
        
        rates_adjusted          = ((1 + interest_rate) / deflator_rate) - 1.0    
        
        if isSteadyEconomy :
            s['debtrates'] = np.array([np.nanmean( rates_adjusted )])
        else:
            s['debtrates'] = rates_adjusted

        
        if isSteadyEconomy :
            s['treas1yr'] = np.nanmean( treas1yr)
            s['treas2yr'] = np.nanmean( treas2yr)
            s['treas3yr'] = np.nanmean( treas3yr)
            s['treas5yr'] = np.nanmean( treas5yr)
            s['treas7yr'] = np.nanmean( treas7yr)
            s['treas10yr'] = np.nanmean( treas10yr)
            s['treas30yr'] = np.nanmean( treas30yr) 
        else:
            s['treas1yr'] = treas1yr
            s['treas2yr'] = treas2yr
            s['treas3yr'] = treas3yr
            s['treas5yr'] = treas5yr
            s['treas7yr'] = treas7yr
            s['treas10yr'] = treas10yr
            s['treas30yr'] = treas30yr
       
        # Personal consumption expenditure price index
        s['steadyStatePriceGrowth'] = steadyStatePriceGrowth
        s['deflator']     = (projections_series['PCEDeflator'] / projections_series['PCEDeflator'][0]) # normalize to 1 for first_year
        PCEdeflator    = np.concatenate(( history_series['PCEDeflator'][0:-T_model], projections_series['PCEDeflator'] ))
        s['longDeflator'] = (PCEdeflator / PCEdeflator[T_life-1])

        # TAX REVENUE AND EXPENDITURE TARGETS           
        s['tax_revenue_by_GDP'] = (revenues / projections_series['GDP_FY'])
        s['outlays_by_GDP']     = (noninterestspending / projections_series['GDP_FY'])
        
        
        # WARNINGS if parameters are outside expectations
        if scenario.isSteady() :
            if s['debttoout'] < 0.6 or s['debttoout'] > 1.0 :
                print( 'WARNING! debttoout=%f outside expectations.\n' % s['debttoout'] )
        for t in range(1,T_model):
            if abs((s['deflator'][t]/s['deflator'][t-1])-1 > 0.05 ) :
                print( 'WARNING! cpe deflator outside expectations. \n' )
        
        # Check if all entries are numerical
        for i in s.keys():
            assert not float('nan') in np.array(s[i]) , 'Series %s contains NaN elements.' % i
                
        return s
    
    
    ## BEQUEST MOTIVE
    #
    def bequest_motive(scenario):
        # phi1 reflects parent's concern about leaving bequests to her children (-9.5 in De Nardi's calibration)
        # phi2 measures the extent to which bequests are a luxury good
        # phi3 is the relative risk aversion coefficient

        
        s = {}
        
        s['phi1'] = scenario.bequest_phi_1   
        s['phi2'] = 11.6                     
        s['phi3'] = 1 - (1 - scenario.sigma) * scenario.gamma    

        return s
        
    ##
    #  OPENNESS OF ECONOMY AND INTERNATIONAL STUFF
    def international(scenario):

        
        timing          = ParamGenerator.timing(scenario)
        first_year      = timing['yearFirst']
        last_year       = timing['yearLast']
        s = {}
        
        # debtTakeUp is the percentage of new debt that is
        # acquired by foreigners. New Debt = D' - D
        # capitalTakeUp is the percentage of 'optimal' capital that
        # is acquired by foreigners. Optimal capital = K_f (open)
        # which is the amount of capital which would be taken up by
        # foreigners in the 'open' economy.
        
        pathFinder      = PathFinder(scenario)
        opennessfile    = pathFinder.getOpennessInputPath( 'Openness' )
        series          = InputReader.read_series(opennessfile, 'Year', first_year, last_year)
        s['capitalTakeUp'] = series['CapitalTakeUp']
        s['debtTakeUp']    = series['DebtTakeUp']
        
        return s

    ## ELASTICITY INVERSION
    # 
    # Invert target elasticities using calibration points
    #   Reusable inverter constructed in the process
    # 
    def invert(targets):
        
        # Note: GDP per adult in 2018 = $86445.71
        # TODO -- revisit the Calibration process
        #   for now just pick one of two hardcoded targets
        # BEGIN TEMP
        try:
            if targets['LaborShock'] == 'Kent9':
                inverse = {'beta':                0.997550000000, 
                           'gamma':               0.687000000000, 
                           'sigma':               1.600000000000, 
                           'bequest_phi_1':       0             , 
                           'modelunit_dollar':    1.77200000e-05 }
            elif targets['LaborShock'] == 'Kent5':
                inverse = {'beta':                1.001000000000, 
                           'gamma':               0.675000000000, 
                           'sigma':               1.600000000000, 
                           'bequest_phi_1':       0             , 
                           'modelunit_dollar':    1.551000000e-05  }
            elif targets['LaborShock'] == 'MichaelJordan10':
                inverse = {'beta'               : 0.994000000000,
                           'gamma'              : 0.721000000000,
                           'sigma'              : 1.600000000000,
                           'bequest_phi_1'      : 0             ,
                           'modelunit_dollar'   : 3.915000000e-05 }
            elif targets['LaborShock'] == 'MichaelJordan12':
                inverse = {'beta'               : 0.977200000000, 
                           'gamma'              : 0.780000000000, 
                           'sigma'              : 1.600000000000, 
                           'bequest_phi_1'      : 0             , 
                           'modelunit_dollar'   : 5.399000000e-05  }

            if 'IsLowReturn' in targets.keys():
                if targets['IsLowReturn']: 
                    print( 'WARNING: Low return economy was not re-calibrated. \n' )
                    inverse = {                         
                            'beta'               : 0.999100000000,         
                            'gamma'              : 0.713000000000,         
                            'sigma'              : 1.600000000000,        
                            'bequest_phi_1'      : 0             ,       
                            'modelunit_dollar'   : 3.79500000e-05     }
        
        except:
            
            if targets.LaborShock == 'Kent9':
                inverse = {'beta':                0.997550000000, 
                           'gamma':               0.687000000000, 
                           'sigma':               1.600000000000, 
                           'bequest_phi_1':       0             , 
                           'modelunit_dollar':    1.77200000e-05 }
            elif targets.LaborShock == 'Kent5':
                inverse = {'beta':                1.001000000000, 
                           'gamma':               0.675000000000, 
                           'sigma':               1.600000000000, 
                           'bequest_phi_1':       0             , 
                           'modelunit_dollar':    1.551000000e-05  }
            elif targets.LaborShock == 'MichaelJordan10':
                inverse = {'beta'               : 0.994000000000,
                           'gamma'              : 0.721000000000,
                           'sigma'              : 1.600000000000,
                           'bequest_phi_1'      : 0             ,
                           'modelunit_dollar'   : 3.915000000e-05 }
            elif targets.LaborShock == 'MichaelJordan12':
                inverse = {'beta'               : 0.977200000000, 
                           'gamma'              : 0.780000000000, 
                           'sigma'              : 1.600000000000, 
                           'bequest_phi_1'      : 0             , 
                           'modelunit_dollar'   : 5.399000000e-05  }

            if hasattr(targets, 'IsLowReturn'):
                if targets.IsLowReturn: 
                    print( 'WARNING: Low return economy was not re-calibrated. \n' )
                    inverse = {                         
                            'beta'               : 0.999100000000,         
                            'gamma'              : 0.713000000000,         
                            'sigma'              : 1.600000000000,        
                            'bequest_phi_1'      : 0             ,       
                            'modelunit_dollar'   : 3.79500000e-05     }
        
        return inverse
        #END TEMP    
    
        # Load calibration points from calibration input directory
        s = scipy.io.loadmat(os.path.join(PathFinder.getCalibrationInputDir(), 'calibration.mat'))
        paramv  = s['paramv'] 
        targetv = s['targetv']
        solved  = s['solved']
        
        # Determine list of calibration parameters
        paramlist = paramv.keys()
        
        # Construct inverse interpolants that map individual target values to calibration parameters
        #   Target values include capital-to-output ratio in addition to elasticities
        interp = {}
        for p in paramlist:
            interp[p] = scipy.interpolate.RegularGridInterpolator(
                (targetv['captoout'][solved], targetv['labelas'][solved], targetv['savelas'][solved]),
                paramv[p][solved], method = 'nearest')
        
        # Construct elasticity inverter by consolidating inverse interpolants
        #   Capital-to-output ratio target fixed at 3
        def f_(targets):
            captoout = 3
            inverse = {}
            for p_ in paramlist:
                inverse[p_] = interp[p_](captoout, targets['labelas'], targets['savelas'])
            
            return inverse
        
        # Invert target elasticities if provided
        if 'targets' in globals():
            inverse = f_(targets)
        else:
            inverse = {}
            
        return inverse
